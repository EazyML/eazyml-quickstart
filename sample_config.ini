[UPLOAD DATA]
#Threshold for Categorical Variables. If the number of unique values is below this threshold, then it will be treated as a #categorical variable
CATEGORICAL_UPPER_THRESHOLD=7

#Threshold for Perturbation. If the number of rows is less than this threshold, then perturbation will be done i.e. data #points will be randomly added
PERTURBATION_THRESHOLD=100.0

#If the outcome column has a count of any 'label' less than this value, then perturbation will be done
PERTURBATION_OUTCOME_LABEL_THRESHOLD=15

[BUILD MODEL]
#The threshold for adding derived predictors. If the number of columns after encoding is less than this threshold, only then #derived predictors are added
DERIVED_PREDICTORS_THRESHOLD=50

#The threshold for adding derived text predictors. If the number of columns after encoding is less than this threshold, only #then text derived predictors are added
DERIVED_TEXT_PREDICTORS_THRESHOLD=100

#The threshold for removing dependent predictors. If the number of columns after encoding is less than this threshold, then #only dependent predictors are removed
VIF_THRESHOLD=50

#Parameter for enabling hyperparameter optimization for the models. If this is set to True, then hyperparameter #optimization will be performed
HYPER_PARAM_OPT_ENABLED=True

#Parameter for enabling different models. If they are set to True, then that particular model is enabled
RANDOM_FOREST_ENABLED=True
BAGGED_DECISION_TREES_ENABLED=True
BOOSTED_DECISION_TREES_ENABLED=True
LINEAR_REGRESSION_ENABLED=True
LASSO_REGRESSION_ENABLED=True
RIDGE_REGRESSION_ENABLED=True
SUPPORT_VECTOR_MACHINE_REGRESSION_ENABLED=True
NEURAL_NETWORK_REGRESSION_ENABLED=True
BAGGED_DECISION_TREES_WITH_INFORMATION_GAIN_ENABLED=True
BOOSTED_DECISION_TREES_WITH_INFORMATION_GAIN_ENABLED=True
RANDOM_FOREST_WITH_INFORMATION_GAIN_ENABLED=True
ADABOOST_DECISION_TREES_ENABLED=True
NAIVE_BAYES_ENABLED=True
NEURAL_NETWORK_CLASSIFICATION_ENABLED=True
SUPPORT_VECTOR_MACHINE_CLASSIFICATION_ENABLED=True
LOGISTIC_REGRESSION_ENABLED=True
K_NEAREST_NEIGHBORS_ENABLED=True

#Parameter for Phrase Split in Concept
DEEP_CONCEPT_PHRASE_SPLIT=False

#Parameter for Fine Match in Concept. If enabled it takes more time
DEEP_CONCEPT_FINE_MATCH=True

#Parameter to call the APIs in any random sequence
API_FLEXIBILITY=False

#The features are sorted in the descending order of their importance and then all the features with cumulative sum less than this threshold are used for predictive models.
PREDICTIVE_FEATURE_SELECT_CUMULATIVE_THRESHOLD = 0.99 

[Explainable AI]
#MIN_R1_SAMPLED_TRAINING_DATA_PROPORTION value  ranges between 0 and 1; recommended to be set over 0.4; The value decides, at minimum how much influence should the prediction point have on the explanation.
MIN_R1_SAMPLED_TRAINING_DATA_PROPORTION = 0.67

#MAX_R1_SAMPLED_TRAINING_DATA_PROPORTION value ranges between 0 and 1; recommended to be set below 0.7; The value sets the high end of influence the prediction point should have on the explanation.
MAX_R1_SAMPLED_TRAINING_DATA_PROPORTION = 0.87

#S2_N2_BETA_CONTEXT_AWARENESS value ranges between 1 and 5 and controls how much weight should the context awareness be given for choice of explanations and corresponding confidence score.
S2_N2_BETA_CONTEXT_AWARENESS = 1

#E1_N1M1_ALPHA_FIT_GOODNESS value ranges between 1 and 5 and controls how much weight should the goodness of the explainer model fit be given for choice of explanations and corresponding confidence score.
E1_N1M1_ALPHA_FIT_GOODNESS = 1

#USE_COMPACT_EXPLANATION value could be set to {True, False}; recommended to be set {False}; If True, then the explainer AI model will be built to produce compact explanations (with fewer rules and conditions).
USE_COMPACT_EXPLANATION = True

#PERTURBATION_TRAINING_DATA_INCREMENT_PROP: Ratio of training data vs perturbations in the mix for building explainer model. The value ranges between 0.005 and 0.05; recommended to be set at 0.01; The value decides, how fine the ratio will vary from minimum to maximum. Lower value implies performance impact.
PERTURBATION_TRAINING_DATA_INCREMENT_PROP = 0.01

#N1_SAMPLES value ranges between 1000 and 5000; recommended to be set at 2000; The value decides, how many points are should you use for minimizing error so that the best explanation is produced.
N1_SAMPLES = 2000

#N2_SAMPLES value ranges between 5000 and 20000; recommended to be set at 10000; The value decides, how many points should you use for maximizing confidence score so that the best explanation is produced.
N2_SAMPLES = 10000

#MULTINOMIAL_HISTOGRAM value can be set to {True, False}; recommended to be set to {True}; If True, then points will be sampled from the PDF of the training data.
MULTINOMIAL_HISTOGRAM = True

#AVG_STD_DEV_OUTCOME value decides, at minimum how much variability should the outcome have for an acceptable explainable AI (default value 0.28).
AVG_STD_DEV_OUTCOME = 0.28

#LIME_FOR_CLASSIF value could be set to {True, False}; recommended to be set {False}; If TRUE, then explanation will be based on LIME for classification problems.
LIME_FOR_CLASSIF = False

#LIME_FOR_REGR value could be set to {True, False}; recommended to be set {False}; If TRUE, then explanation will be based on LIME for regression problems.
LIME_FOR_REGR = False


[Augmented Intelligence]
#The features are sorted in the descending order of their importance and then all the features with cumulative sum less than this threshold are used for augmented intelligence models.
AUGI_FEATURE_SELECT_CUMULATIVE_THRESHOLD = 0.90

#AUGI_N_MODELS value ranges from 1-6 and specifies how diverse the insights should be in terms of features used in the explanations of human readable rules and conditions. Lower value implies faster computation of augmented intelligence insights. Higher value implies rules with diverse features but consumes more computation time.
AUGI_N_MODELS = 5

#AUGI_DENSITY_PCT value ranges from 0.01 to 0.10 and specifies how compact the insights should be. Higher the value, the more compact the insights and faster the computation. Lower the value, the more detailed the insights and slower the computation.
AUGI_DENSITY_PCT = 0.00001

#AUGI_KN_THRESHOLD_MIN value specifies the minimum confidence score for insights over which the insights are considered acceptable. The value ranges from 0.77 to 1. (If you set it to 1, only insights with 100% confidence are considered acceptable)
AUGI_KN_THRESHOLD_MIN = 0.62

#AUGI_KN_THRESHOLD_MAX value specifies the minimum confidence score for insights over which the insights are considered acceptable. The value ranges from 0.77 to 1. (If you set it to 1, only insights with 100% confidence are considered acceptable)
AUGI_KN_THRESHOLD_MAX = 0.77

#AUGI_B_PHAT_BPS_* value tunes what is your acceptable range for confidence scores â€“ for datasets of sizes commonly used in your enterprise. The default values is set as follows. You may configure the penalty as per the table and set AUGI_PHAT_USER_PENALTY to override the default penalty.
#B_PHAT_SCORE -> PENALTY FACTOR
#BPS_0.1      ->       1       
#BPS_0.2      ->     0.99      
#BPS_0.3      ->     0.97      
#BPS_0.4      ->     0.92      
#BPS_0.5      ->     0.90      
#BPS_0.5+     ->     0.88      

AUGI_PHAT_USER_PENALTY = False
AUGI_B_PHAT_BPS_01 = 1.0
AUGI_B_PHAT_BPS_02 = 0.99
AUGI_B_PHAT_BPS_03 = 0.97
AUGI_B_PHAT_BPS_04 = 0.92
AUGI_B_PHAT_BPS_05 = 0.90
AUGI_B_PHAT_BPS_05_PLUS = 0.88

#AUGI_INFLATE_ACCURACY_SCORE value inflates the accuracy score. Set it to False to use the actual accuracy in score.
AUGI_INFLATE_ACCURACY_SCORE = True

#Parameters to override auto-computed values of Sigmoid parameters used in scoring model for accuracy and coverage in data-insights and local transparency. Please keep AUGI_USER_SCORE to False if you want to use the values auto-computed by EazyML instead.
AUGI_USER_SCORE = False
AUGI_ALPHA_COV = 0.045 
AUGI_BETA_COV = 33.9
AUGI_ALPHA_ACC = 0.59
AUGI_BETA_ACC = 12
